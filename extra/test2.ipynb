{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit"
  },
  "interpreter": {
   "hash": "900f0694acc064964699e78a6edfb089db62b200ad88e30afa87c26005b6686c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import DateType,StringType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import avg\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df = spark.read.parquet(os.path.join(os.getpwd(), '/*.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_df = spark.read.option(\"multiline\",\"true\").json('sample_data/us-cities-demographics.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_df.filter(demographics_df.City == \"New York\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create demographics dimension table\n",
    "demographics_table = demographics_df.select(\n",
    "        monotonically_increasing_id().alias('city_id'),\n",
    "        col('City').alias('city_name'),\n",
    "        col('State').alias('state_name'),\n",
    "        col('Median Age').alias('median_age'),\n",
    "        col('Male Population').alias('male_population'),\n",
    "        col('Female Population').alias('female_population'),\n",
    "        col('Total Population').alias('total_population'),\n",
    "        col('Number of Veterans').alias('num_veterans'),\n",
    "        col('Foreign-born').alias('foreign_born'),\n",
    "        col('Average Household Size').alias('avg_household'),\n",
    "        col('State Code').alias('state_code'),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures_df = spark.read.csv('sample_data/temperatures_sample.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing average temperature\n",
    "temperatures_df = temperatures_df.dropna(subset=['AverageTemperature'])\n",
    "    \n",
    "# drop duplicate rows\n",
    "temperatures_df = temperatures_df.drop_duplicates(subset=['dt', 'City', 'Country'])\n",
    "\n",
    "# filter city in US\n",
    "temperatures_df = temperatures_df.filter(temperatures_df.Country == 'United States')\n",
    "\n",
    "temperatures_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = temperatures_df.join(demographics_table, (temperatures_df.City == demographics_table.city_name), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df = spark.read.csv('sample_data/airport-codes_csv.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df = airports_df.filter(airports_df.continent == 'NA')\n",
    "airports_df = airports_df.filter(airports_df.iso_country == 'US')\n",
    "\n",
    "# extract 2-letter state code\n",
    "extract_state_code = F.udf(lambda x: x[3:], StringType())\n",
    "airports_df = airports_df.withColumn('state_code', extract_state_code('iso_region'))\n",
    "\n",
    "# extract columns to create songs table\n",
    "airports_table = airports_df.select(\n",
    "        col('ident').alias('airport_code'),\n",
    "        'state_code',\n",
    "        'type',\n",
    "        'name',\n",
    "        col('municipality').alias('city')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df = airports_df.filter(airports_df.continent == 'NA')\n",
    "airports_df = airports_df.filter(airports_df.iso_country == 'US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures_table = temperatures_df.select(\n",
    "        monotonically_increasing_id().alias('temperature_id'),\n",
    "        col('AverageTemperature').alias('avg_temp'),\n",
    "        col('City').alias('city'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_state_code = F.udf(lambda x: x[3:], StringType())\n",
    "airports_df = airports_df.withColumn('state_code', extract_state_code('iso_region'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_2_df = temperatures_table.join(airports_table,(temperatures_table.City==airports_table.),how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}